{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ebe599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Airlines             Name Date Published  \\\n",
      "0        Qatar Airways     Romana Malik     2025-09-07   \n",
      "1        Qatar Airways           J Raiz     2025-09-02   \n",
      "2        Qatar Airways       Iman Yusuf     2025-09-01   \n",
      "3        Qatar Airways     Ronald Zwart     2025-08-26   \n",
      "4        Qatar Airways  Dmitriy Berezin     2025-08-21   \n",
      "...                ...              ...            ...   \n",
      "14586  Hainan Airlines        J Depaepe     2010-01-19   \n",
      "14587  Hainan Airlines  Pieter D'Hamers     2010-01-10   \n",
      "14588  Hainan Airlines           Y Chen     2010-01-09   \n",
      "14589  Hainan Airlines          A Smith     2009-12-16   \n",
      "14590  Hainan Airlines    Richard Borst     2009-12-11   \n",
      "\n",
      "                                            Text Content  \n",
      "0      we choose our seats when booking and they chan...  \n",
      "1      initially i was supposed to be traveling with ...  \n",
      "2      i want to sincerely thank qatar airways for th...  \n",
      "3      boarding was efficient friendly personable wel...  \n",
      "4      when booking the flight wa shown as a qsuite b...  \n",
      "...                                                  ...  \n",
      "14586  brupek business class 6 hour delay in brussels...  \n",
      "14587  brupekbru new a330 good and friendly service v...  \n",
      "14588  beijingguangzhou b737800 in economy despite th...  \n",
      "14589  xiy to ctu on boeing 737800 in economy flight ...  \n",
      "14590  beijing to xian on hainan airlines very good s...  \n",
      "\n",
      "[14525 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load data\n",
    "url = 'https://raw.githubusercontent.com/DyanelLancea/Airline-Review-Sentiement-Analysis/refs/heads/master/airlines_review.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "\n",
    "# List of special characters to remove\n",
    "removechar = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')',\n",
    "               '-', '_', '=', '+', '{', '}', '[', ']', '|',\n",
    "               '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.', '?',\n",
    "                 '/', '~', '`', '✅ Trip Verified', 'Not Verified', 'Â Â']\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Replace missing values with NA or Unknown function\n",
    "def replace_missing_value(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':  # Object type (string)\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"NA\")\n",
    "    return \n",
    "\n",
    "# Remove special characters from specific columns function\n",
    "def remove_special_characters(df, removechar, char):\n",
    "    for char in removechar:\n",
    "        df['Airlines'] = df['Airlines'].str.replace(char, ' ', regex=False)\n",
    "        df['Text Content'] = df['Text Content'].str.replace(char, '', regex=False)\n",
    "    return df\n",
    "\n",
    "# Apply data cleaning functions\n",
    "replace_missing_value(df)\n",
    "remove_special_characters(df, removechar, char='')\n",
    "\n",
    "# Standardize text case\n",
    "df['Airlines'] = df['Airlines'].str.title()\n",
    "df['Name'] = df['Name'].str.title()\n",
    "df['Text Content'] = df['Text Content'].str.lower()\n",
    "\n",
    "# Remove leading spaces from the 'Name' column\n",
    "df['Airlines'] = df['Airlines'].str.lstrip()\n",
    "df['Name'] = df['Name'].str.lstrip()\n",
    "df['Date Published'] = df['Date Published'].str.lstrip()\n",
    "df['Text Content'] = df['Text Content'].str.lstrip()\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv('airlines_review_cleaned.csv', index=False)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dca0a4",
   "metadata": {},
   "source": [
    "### Function to read and parse the AFINN lexicon, then loads it into a dictionary (afinn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e831f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_afinn_lexicon(afinn_url):\n",
    "    afinn_dict = {} # initialise an empty dictionary to hold the AFINN lexicon\n",
    "    response = requests.get(afinn_url)  # download the file\n",
    "    try:\n",
    "        for line in response.text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line: # checks for and skips any empty lines in the file\n",
    "                continue\n",
    "            word, score = line.rsplit('\\t', 1) # splits the line into two parts\n",
    "            afinn_dict[word] = int(score) # adds the word and its score to the dict\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {afinn_url}\")\n",
    "    return afinn_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9066c",
   "metadata": {},
   "source": [
    "### Load the AFINN lexicon (provided sentiment dictionary) then print a sample of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFINN dictionary loaded. Sample entries:\n",
      "[('abandon', -2), ('abandoned', -2), ('abandons', -2), ('abducted', -2), ('abduction', -2), ('abductions', -2), ('abhor', -3), ('abhorred', -3), ('abhorrent', -3), ('abhors', -3)]\n"
     ]
    }
   ],
   "source": [
    "afinn_url = \"https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\"\n",
    "afinn_dict = load_afinn_lexicon(afinn_url)\n",
    "\n",
    "print(\"AFINN dictionary loaded. Sample entries:\")\n",
    "print(list(afinn_dict.items())[:10])  # show first 10 words and scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77788de",
   "metadata": {},
   "source": [
    "### Function for sentence tokanization \n",
    "This function takes some text and returns a list of sentences. If you don’t give it a string, it safely returns an empty list instead of crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae96bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85552f86",
   "metadata": {},
   "source": [
    "### Function to calculate sentiment and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6035bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_score(sentence, afinn_dict):\n",
    "    score = 0\n",
    "    words = sentence.lower().split()    # breaks down the sentence into individual words\n",
    "    for word in words:  # goes through each word in the list \n",
    "        score += afinn_dict.get(word, 0)    # looks up each word and adds the score\n",
    "    return score\n",
    "\n",
    "def normalize_score(score, text_length):\n",
    "    if text_length == 0:    # sentences with no words will return a score of 0 (preventing an error of dividing by zero)\n",
    "        return 0\n",
    "    # normalization to get a score per word, then clamping to [-1, 1]\n",
    "    normalized = score / text_length    # calculates the average score per word\n",
    "    return max(-1.0, min(1.0, normalized))  \n",
    "    # makes sure the score doesn't go over 1.0 and below -1.0. \n",
    "    # (if the score is -2.5, this will return -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2cf7",
   "metadata": {},
   "source": [
    "### Function to find the extreme sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03342e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extreme_sentences(sentences, afinn_dict):\n",
    "    \n",
    "    if not sentences:   # to prevent errors if the list is empty\n",
    "        return None, None\n",
    "\n",
    "    # initialises an empty list to hold sentences and their scores\n",
    "    scored_sentences = []\n",
    "    for sent in sentences:\n",
    "        score = calculate_sentiment_score(sent, afinn_dict)     # uses the sentiment score function\n",
    "        normalized_score = normalize_score(score, len(sent.split()))    # uses the normalization function\n",
    "        scored_sentences.append({'sentence': sent, 'score': normalized_score})\n",
    "\n",
    "    # find the sentences with max and min normalized scores\n",
    "    most_positive = max(scored_sentences, key=lambda x: x['score'])\n",
    "    most_negative = min(scored_sentences, key=lambda x: x['score'])\n",
    "    \n",
    "    return most_positive, most_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c05a78",
   "metadata": {},
   "source": [
    "### Sliding Window Function to find the most positive and negative paragraphs.\n",
    "\n",
    "Using a window size lets us \"slide\" a focus area over the text. The window's score changes as it moves, so it can detect specific pockets of strong emotion(positive/negative).\n",
    "\n",
    "When the window is over a section with very positive sentences, it will get a high score. When it slides over the negative section, it will get a very low score.\n",
    "\n",
    "This give us a much more detailed and accurate picture of the sentiment throughout the text. It helps us pinpoint exactly where the most positive and negative opinions are located, rather than just getting a single, overall average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26728f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_analysis_words(text, afinn_dict, window_size=10):\n",
    "    \"\"\"\n",
    "    Applies a sliding window to find the most positive and negative text segments,\n",
    "    based on a word-level window size.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text to analyze.\n",
    "        afinn_dict (dict): The AFINN sentiment lexicon.\n",
    "        window_size (int): The number of words to include in each window.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the most positive and most negative paragraphs.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None, None\n",
    "        \n",
    "    words = text.lower().split()\n",
    "    if len(words) < window_size:\n",
    "        return None, None\n",
    "    \n",
    "    scored_windows = []\n",
    "    # Slide the window across the words\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = words[i:i + window_size]\n",
    "        paragraph_text = ' '.join(window)\n",
    "        \n",
    "        # Calculate score for the paragraph window\n",
    "        score = calculate_sentiment_score(paragraph_text, afinn_dict)\n",
    "        normalized_score = normalize_score(score, len(paragraph_text.split()))\n",
    "\n",
    "        scored_windows.append({'paragraph': paragraph_text, 'score': normalized_score})\n",
    "\n",
    "    if not scored_windows:\n",
    "        return None, None\n",
    "\n",
    "    # Find the paragraphs with max and min scores\n",
    "    most_positive_paragraph = max(scored_windows, key=lambda x: x['score'])\n",
    "    most_negative_paragraph = min(scored_windows, key=lambda x: x['score'])\n",
    "    \n",
    "    return most_positive_paragraph, most_negative_paragraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
