{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16ebe599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\daryl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\daryl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Airlines             Name Date Published  \\\n",
      "0        Qatar Airways     Romana Malik     2025-09-07   \n",
      "1        Qatar Airways           J Raiz     2025-09-02   \n",
      "2        Qatar Airways       Iman Yusuf     2025-09-01   \n",
      "3        Qatar Airways     Ronald Zwart     2025-08-26   \n",
      "4        Qatar Airways  Dmitriy Berezin     2025-08-21   \n",
      "...                ...              ...            ...   \n",
      "14586  Hainan Airlines        J Depaepe     2010-01-19   \n",
      "14587  Hainan Airlines  Pieter D'Hamers     2010-01-10   \n",
      "14588  Hainan Airlines           Y Chen     2010-01-09   \n",
      "14589  Hainan Airlines          A Smith     2009-12-16   \n",
      "14590  Hainan Airlines    Richard Borst     2009-12-11   \n",
      "\n",
      "                                            Text Content  \n",
      "0      we choose our seats when booking and they chan...  \n",
      "1      initially i was supposed to be traveling with ...  \n",
      "2      i want to sincerely thank qatar airways for th...  \n",
      "3      boarding was efficient friendly personable wel...  \n",
      "4      when booking the flight wa shown as a qsuite b...  \n",
      "...                                                  ...  \n",
      "14586  brupek business class. 6 hour delay in brussel...  \n",
      "14587  brupekbru. new a330 good and friendly service....  \n",
      "14588  beijingguangzhou b737800 in economy. despite t...  \n",
      "14589  xiy to ctu on boeing 737800 in economy. flight...  \n",
      "14590  beijing to xian on hainan airlines. very good ...  \n",
      "\n",
      "[14525 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load data\n",
    "url = 'https://raw.githubusercontent.com/DyanelLancea/Airline-Review-Sentiement-Analysis/refs/heads/master/airlines_review.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "\n",
    "# List of special characters to remove\n",
    "removechar = [ '@', '#', '$', '%', '^', '&', '*', '(', ')',\n",
    "               '-', '_', '=', '+', '{', '}', '[', ']', '|',\n",
    "               '\\\\', ':', ';', '\"', \"'\", '<', '>', ',',\n",
    "                 '/', '~', '`', '✅ Trip Verified', 'Not Verified', 'Â Â']\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Replace missing values with NA or Unknown function\n",
    "def replace_missing_value(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':  # Object type (string)\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"NA\")\n",
    "    return \n",
    "\n",
    "# Remove special characters from specific columns function\n",
    "def remove_special_characters(df, removechar, char):\n",
    "    for char in removechar:\n",
    "        df['Airlines'] = df['Airlines'].str.replace(char, ' ', regex=False)\n",
    "        df['Text Content'] = df['Text Content'].str.replace(char, '', regex=False)\n",
    "    return df\n",
    "\n",
    "# Apply data cleaning functions\n",
    "replace_missing_value(df)\n",
    "remove_special_characters(df, removechar, char='')\n",
    "\n",
    "# Standardize text case\n",
    "df['Airlines'] = df['Airlines'].str.title()\n",
    "df['Name'] = df['Name'].str.title()\n",
    "df['Text Content'] = df['Text Content'].str.lower()\n",
    "\n",
    "# Remove leading spaces from the 'Name' column\n",
    "df['Airlines'] = df['Airlines'].str.lstrip()\n",
    "df['Name'] = df['Name'].str.lstrip()\n",
    "df['Date Published'] = df['Date Published'].str.lstrip()\n",
    "df['Text Content'] = df['Text Content'].str.lstrip()\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df.to_csv('airlines_review_cleaned.csv', index=False)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dca0a4",
   "metadata": {},
   "source": [
    "### Function to read and parse the AFINN lexicon, then loads it into a dictionary (afinn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83e831f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_afinn_lexicon(afinn_url):\n",
    "    afinn_dict = {} # initialise an empty dictionary to hold the AFINN lexicon\n",
    "    response = requests.get(afinn_url)  # download the file\n",
    "    try:\n",
    "        for line in response.text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line: # checks for and skips any empty lines in the file\n",
    "                continue\n",
    "            word, score = line.rsplit('\\t', 1) # splits the line into two parts\n",
    "            afinn_dict[word] = int(score) # adds the word and its score to the dict\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {afinn_url}\")\n",
    "    return afinn_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9066c",
   "metadata": {},
   "source": [
    "### Load the AFINN lexicon (provided sentiment dictionary) then print a sample of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fd5d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFINN dictionary loaded. Sample entries:\n",
      "[('abandon', -2), ('abandoned', -2), ('abandons', -2), ('abducted', -2), ('abduction', -2), ('abductions', -2), ('abhor', -3), ('abhorred', -3), ('abhorrent', -3), ('abhors', -3)]\n"
     ]
    }
   ],
   "source": [
    "afinn_url = \"https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\"\n",
    "afinn_dict = load_afinn_lexicon(afinn_url)\n",
    "\n",
    "print(\"AFINN dictionary loaded. Sample entries:\")\n",
    "print(list(afinn_dict.items())[:10])  # show first 10 words and scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77788de",
   "metadata": {},
   "source": [
    "### Function for sentence tokanization \n",
    "This function takes some text and returns a list of sentences. If you don’t give it a string, it safely returns an empty list instead of crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ae96bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85552f86",
   "metadata": {},
   "source": [
    "### Function to calculate sentiment and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6035bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_score(sentences, afinn_dict):\n",
    "    score = 0\n",
    "    words = sentences.lower().split()    # breaks down the sentence into individual words\n",
    "    for word in words:  # goes through each word in the list \n",
    "        score += afinn_dict.get(word, 0)    # looks up each word and adds the score\n",
    "    return score\n",
    "\n",
    "def normalize_score(score, text_length):\n",
    "    if text_length == 0:    # sentences with no words will return a score of 0 (preventing an error of dividing by zero)\n",
    "        return 0\n",
    "    # normalization to get a score per word, then clamping to [-1, 1]\n",
    "    normalized = score / text_length    # calculates the average score per word\n",
    "    return max(-1.0, min(1.0, normalized))  \n",
    "    # makes sure the score doesn't go over 1.0 and below -1.0. \n",
    "    # (if the score is -2.5, this will return -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb2cf7",
   "metadata": {},
   "source": [
    "### Function to find the extreme sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03342e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extreme_sentences(sentences, afinn_dict):\n",
    "    \n",
    "    if not sentences:   # to prevent errors if the list is empty\n",
    "        return None, None\n",
    "\n",
    "    # initialises an empty list to hold sentences and their scores\n",
    "    scored_sentences = []\n",
    "    for sent in sentences:\n",
    "        score = calculate_sentiment_score(sent, afinn_dict)     # uses the sentiment score function\n",
    "        normalized_score = normalize_score(score, len(sent.split()))    # uses the normalization function\n",
    "        scored_sentences.append({'sentence': sent, 'score': normalized_score})\n",
    "\n",
    "    # find the sentences with max and min normalized scores\n",
    "    most_positive = max(scored_sentences, key=lambda x: x['score'])\n",
    "    most_negative = min(scored_sentences, key=lambda x: x['score'])\n",
    "    \n",
    "    return most_positive, most_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c05a78",
   "metadata": {},
   "source": [
    "### Sliding Window Function to find the most positive and negative paragraphs.\n",
    "\n",
    "Using a window size lets us \"slide\" a focus area over the text. The window's score changes as it moves, so it can detect specific pockets of strong emotion(positive/negative).\n",
    "\n",
    "When the window is over a section with very positive sentences, it will get a high score. When it slides over the negative section, it will get a very low score.\n",
    "\n",
    "This give us a much more detailed and accurate picture of the sentiment throughout the text. It helps us pinpoint exactly where the most positive and negative opinions are located, rather than just getting a single, overall average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26728f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_analysis(sentences, afinn_dict, window_size=3):\n",
    "    # window size is how many sentences to include in each paragraph which is 3\n",
    "    \n",
    "    if len(sentences) < window_size:    # checks if enough sentences exists\n",
    "        return None, None\n",
    "    \n",
    "    # initialises an empty list to hold sentences and their scores\n",
    "    scored_windows = []\n",
    "    for i in range(len(sentences) - window_size + 1):    # slides the window across the sentences\n",
    "        window = sentences[i:i + window_size]\n",
    "        paragraph_text = ' '.join(window)\n",
    "        \n",
    "        # calculate score for the paragraph window\n",
    "        score = calculate_sentiment_score(paragraph_text, afinn_dict)   # uses the sentiment score function\n",
    "        normalized_score = normalize_score(score, len(paragraph_text.split()))  # uses the normalization function\n",
    "\n",
    "        # qdds a dictionary with two things: the paragraph text and its normalized score\n",
    "        scored_windows.append({'paragraph': paragraph_text, 'score': normalized_score})\n",
    "\n",
    "    # find the paragraphs with max and min scores\n",
    "    most_positive_paragraph = max(scored_windows, key=lambda x: x['score'])\n",
    "    most_negative_paragraph = min(scored_windows, key=lambda x: x['score'])\n",
    "    \n",
    "    return most_positive_paragraph, most_negative_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0277cd9",
   "metadata": {},
   "source": [
    "Applying functions into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe2f19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply tonkenize function into Dataframe\n",
    "df['Text Content Tokenized'] = df['Text Content'].apply(tokenize_sentences)\n",
    "#Apply Sentiment scoring function into Dataframe\n",
    "df['Sentiment Score'] = df.apply(lambda x: calculate_sentiment_score(x['Text Content'],afinn_dict), axis=1)\n",
    "#Apply normalize function to Dataframe\n",
    "df['Normalized Sentiment Score'] = df.apply(lambda x: normalize_score(x['Sentiment Score'],len(x[\"Text Content\"].split())), axis=1)\n",
    "#Apply finding extreme sentences function to Dataframe\n",
    "df['Extreme Senctences'] = df.apply(lambda x: find_extreme_sentences(x['Text Content Tokenized'],afinn_dict), axis=1)\n",
    "#Creating Columns for Most and Least Extreme Sentences\n",
    "df['Most Positive Senctence'] = df['Extreme Senctences'].apply(lambda x: x[0]['sentence'])\n",
    "df['Most Positive Senctence Score'] = df['Extreme Senctences'].apply(lambda x: x[0]['score'])\n",
    "df['Most Negative Senctence'] = df['Extreme Senctences'].apply(lambda x: x[1]['sentence'])\n",
    "df['Most Negative Senctence Score'] = df['Extreme Senctences'].apply(lambda x: x[1]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "67612d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airlines</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date Published</th>\n",
       "      <th>Text Content</th>\n",
       "      <th>Text Content Tokenized</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Normalized Sentiment Score</th>\n",
       "      <th>most positive senctence</th>\n",
       "      <th>Extreme Senctences</th>\n",
       "      <th>Most Positive Senctence</th>\n",
       "      <th>Most Positive Senctence Score</th>\n",
       "      <th>Most Negative Senctence</th>\n",
       "      <th>Most Negative Senctence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>Romana Malik</td>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>we choose our seats when booking and they chan...</td>\n",
       "      <td>[we choose our seats when booking and they cha...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>({'sentence': 'fortunately we got the bulkhead...</td>\n",
       "      <td>({'sentence': 'fortunately we got the bulkhead...</td>\n",
       "      <td>fortunately we got the bulkhead seats we had s...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>we choose our seats when booking and they chan...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>J Raiz</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>initially i was supposed to be traveling with ...</td>\n",
       "      <td>[initially i was supposed to be traveling with...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>({'sentence': 'i have been a loyal client.', '...</td>\n",
       "      <td>({'sentence': 'i have been a loyal client.', '...</td>\n",
       "      <td>i have been a loyal client.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>initially i was supposed to be traveling with ...</td>\n",
       "      <td>-0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>Iman Yusuf</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>i want to sincerely thank qatar airways for th...</td>\n",
       "      <td>[i want to sincerely thank qatar airways for t...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>({'sentence': 'i want to sincerely thank qatar...</td>\n",
       "      <td>({'sentence': 'i want to sincerely thank qatar...</td>\n",
       "      <td>i want to sincerely thank qatar airways for th...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>while checking in at jeddah airport i couldn’t...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>Ronald Zwart</td>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>boarding was efficient friendly personable wel...</td>\n",
       "      <td>[boarding was efficient friendly personable we...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>({'sentence': 'boarding was efficient friendly...</td>\n",
       "      <td>({'sentence': 'boarding was efficient friendly...</td>\n",
       "      <td>boarding was efficient friendly personable wel...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>naturally the mandatory photo standing behind ...</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qatar Airways</td>\n",
       "      <td>Dmitriy Berezin</td>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>when booking the flight wa shown as a qsuite b...</td>\n",
       "      <td>[when booking the flight wa shown as a qsuite ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.018868</td>\n",
       "      <td>({'sentence': 'customer care has been unrespon...</td>\n",
       "      <td>({'sentence': 'customer care has been unrespon...</td>\n",
       "      <td>customer care has been unresponsive after mult...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>when booking the flight wa shown as a qsuite b...</td>\n",
       "      <td>-0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>Hainan Airlines</td>\n",
       "      <td>J Depaepe</td>\n",
       "      <td>2010-01-19</td>\n",
       "      <td>brupek business class. 6 hour delay in brussel...</td>\n",
       "      <td>[brupek business class., 6 hour delay in bruss...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>({'sentence': 'new aircraft very personal serv...</td>\n",
       "      <td>({'sentence': 'new aircraft very personal serv...</td>\n",
       "      <td>new aircraft very personal service great food ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>6 hour delay in brussels but flight was just p...</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587</th>\n",
       "      <td>Hainan Airlines</td>\n",
       "      <td>Pieter D'Hamers</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>brupekbru. new a330 good and friendly service....</td>\n",
       "      <td>[brupekbru., new a330 good and friendly servic...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>({'sentence': 'new a330 good and friendly serv...</td>\n",
       "      <td>({'sentence': 'new a330 good and friendly serv...</td>\n",
       "      <td>new a330 good and friendly service.</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>brupekbru.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>Hainan Airlines</td>\n",
       "      <td>Y Chen</td>\n",
       "      <td>2010-01-09</td>\n",
       "      <td>beijingguangzhou b737800 in economy. despite t...</td>\n",
       "      <td>[beijingguangzhou b737800 in economy., despite...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>({'sentence': 'excellent service by flight att...</td>\n",
       "      <td>({'sentence': 'excellent service by flight att...</td>\n",
       "      <td>excellent service by flight attendants as they...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>despite the 30 minute delay hna did well to in...</td>\n",
       "      <td>-0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>Hainan Airlines</td>\n",
       "      <td>A Smith</td>\n",
       "      <td>2009-12-16</td>\n",
       "      <td>xiy to ctu on boeing 737800 in economy. flight...</td>\n",
       "      <td>[xiy to ctu on boeing 737800 in economy., flig...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>({'sentence': 'flight delayed one and a half h...</td>\n",
       "      <td>({'sentence': 'flight delayed one and a half h...</td>\n",
       "      <td>flight delayed one and a half hours due to wea...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>xiy to ctu on boeing 737800 in economy.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>Hainan Airlines</td>\n",
       "      <td>Richard Borst</td>\n",
       "      <td>2009-12-11</td>\n",
       "      <td>beijing to xian on hainan airlines. very good ...</td>\n",
       "      <td>[beijing to xian on hainan airlines., very goo...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>({'sentence': 'very good service on time depar...</td>\n",
       "      <td>({'sentence': 'very good service on time depar...</td>\n",
       "      <td>very good service on time departures.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>beijing to xian on hainan airlines.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14525 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Airlines             Name Date Published  \\\n",
       "0        Qatar Airways     Romana Malik     2025-09-07   \n",
       "1        Qatar Airways           J Raiz     2025-09-02   \n",
       "2        Qatar Airways       Iman Yusuf     2025-09-01   \n",
       "3        Qatar Airways     Ronald Zwart     2025-08-26   \n",
       "4        Qatar Airways  Dmitriy Berezin     2025-08-21   \n",
       "...                ...              ...            ...   \n",
       "14586  Hainan Airlines        J Depaepe     2010-01-19   \n",
       "14587  Hainan Airlines  Pieter D'Hamers     2010-01-10   \n",
       "14588  Hainan Airlines           Y Chen     2010-01-09   \n",
       "14589  Hainan Airlines          A Smith     2009-12-16   \n",
       "14590  Hainan Airlines    Richard Borst     2009-12-11   \n",
       "\n",
       "                                            Text Content  \\\n",
       "0      we choose our seats when booking and they chan...   \n",
       "1      initially i was supposed to be traveling with ...   \n",
       "2      i want to sincerely thank qatar airways for th...   \n",
       "3      boarding was efficient friendly personable wel...   \n",
       "4      when booking the flight wa shown as a qsuite b...   \n",
       "...                                                  ...   \n",
       "14586  brupek business class. 6 hour delay in brussel...   \n",
       "14587  brupekbru. new a330 good and friendly service....   \n",
       "14588  beijingguangzhou b737800 in economy. despite t...   \n",
       "14589  xiy to ctu on boeing 737800 in economy. flight...   \n",
       "14590  beijing to xian on hainan airlines. very good ...   \n",
       "\n",
       "                                  Text Content Tokenized  Sentiment Score  \\\n",
       "0      [we choose our seats when booking and they cha...                2   \n",
       "1      [initially i was supposed to be traveling with...                7   \n",
       "2      [i want to sincerely thank qatar airways for t...               12   \n",
       "3      [boarding was efficient friendly personable we...               18   \n",
       "4      [when booking the flight wa shown as a qsuite ...               -1   \n",
       "...                                                  ...              ...   \n",
       "14586  [brupek business class., 6 hour delay in bruss...                2   \n",
       "14587  [brupekbru., new a330 good and friendly servic...                8   \n",
       "14588  [beijingguangzhou b737800 in economy., despite...                2   \n",
       "14589  [xiy to ctu on boeing 737800 in economy., flig...                4   \n",
       "14590  [beijing to xian on hainan airlines., very goo...                5   \n",
       "\n",
       "       Normalized Sentiment Score  \\\n",
       "0                        0.037037   \n",
       "1                        0.106061   \n",
       "2                        0.137931   \n",
       "3                        0.132353   \n",
       "4                       -0.018868   \n",
       "...                           ...   \n",
       "14586                    0.035088   \n",
       "14587                    0.137931   \n",
       "14588                    0.062500   \n",
       "14589                    0.045977   \n",
       "14590                    0.227273   \n",
       "\n",
       "                                 most positive senctence  \\\n",
       "0      ({'sentence': 'fortunately we got the bulkhead...   \n",
       "1      ({'sentence': 'i have been a loyal client.', '...   \n",
       "2      ({'sentence': 'i want to sincerely thank qatar...   \n",
       "3      ({'sentence': 'boarding was efficient friendly...   \n",
       "4      ({'sentence': 'customer care has been unrespon...   \n",
       "...                                                  ...   \n",
       "14586  ({'sentence': 'new aircraft very personal serv...   \n",
       "14587  ({'sentence': 'new a330 good and friendly serv...   \n",
       "14588  ({'sentence': 'excellent service by flight att...   \n",
       "14589  ({'sentence': 'flight delayed one and a half h...   \n",
       "14590  ({'sentence': 'very good service on time depar...   \n",
       "\n",
       "                                      Extreme Senctences  \\\n",
       "0      ({'sentence': 'fortunately we got the bulkhead...   \n",
       "1      ({'sentence': 'i have been a loyal client.', '...   \n",
       "2      ({'sentence': 'i want to sincerely thank qatar...   \n",
       "3      ({'sentence': 'boarding was efficient friendly...   \n",
       "4      ({'sentence': 'customer care has been unrespon...   \n",
       "...                                                  ...   \n",
       "14586  ({'sentence': 'new aircraft very personal serv...   \n",
       "14587  ({'sentence': 'new a330 good and friendly serv...   \n",
       "14588  ({'sentence': 'excellent service by flight att...   \n",
       "14589  ({'sentence': 'flight delayed one and a half h...   \n",
       "14590  ({'sentence': 'very good service on time depar...   \n",
       "\n",
       "                                 Most Positive Senctence  \\\n",
       "0      fortunately we got the bulkhead seats we had s...   \n",
       "1                            i have been a loyal client.   \n",
       "2      i want to sincerely thank qatar airways for th...   \n",
       "3      boarding was efficient friendly personable wel...   \n",
       "4      customer care has been unresponsive after mult...   \n",
       "...                                                  ...   \n",
       "14586  new aircraft very personal service great food ...   \n",
       "14587                new a330 good and friendly service.   \n",
       "14588  excellent service by flight attendants as they...   \n",
       "14589  flight delayed one and a half hours due to wea...   \n",
       "14590              very good service on time departures.   \n",
       "\n",
       "       Most Positive Senctence Score  \\\n",
       "0                           0.153846   \n",
       "1                           0.500000   \n",
       "2                           0.375000   \n",
       "3                           0.444444   \n",
       "4                           0.083333   \n",
       "...                              ...   \n",
       "14586                       0.300000   \n",
       "14587                       0.833333   \n",
       "14588                       0.300000   \n",
       "14589                       0.090909   \n",
       "14590                       0.500000   \n",
       "\n",
       "                                 Most Negative Senctence  \\\n",
       "0      we choose our seats when booking and they chan...   \n",
       "1      initially i was supposed to be traveling with ...   \n",
       "2      while checking in at jeddah airport i couldn’t...   \n",
       "3      naturally the mandatory photo standing behind ...   \n",
       "4      when booking the flight wa shown as a qsuite b...   \n",
       "...                                                  ...   \n",
       "14586  6 hour delay in brussels but flight was just p...   \n",
       "14587                                         brupekbru.   \n",
       "14588  despite the 30 minute delay hna did well to in...   \n",
       "14589            xiy to ctu on boeing 737800 in economy.   \n",
       "14590                beijing to xian on hainan airlines.   \n",
       "\n",
       "       Most Negative Senctence Score  \n",
       "0                           0.000000  \n",
       "1                          -0.068966  \n",
       "2                           0.000000  \n",
       "3                          -0.100000  \n",
       "4                          -0.057143  \n",
       "...                              ...  \n",
       "14586                      -0.100000  \n",
       "14587                       0.000000  \n",
       "14588                      -0.058824  \n",
       "14589                       0.000000  \n",
       "14590                       0.000000  \n",
       "\n",
       "[14525 rows x 13 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
